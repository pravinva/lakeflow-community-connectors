# ==============================================================================
# Merged Lakeflow Source: osipi
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta, timezone
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Optional,
    Tuple,
)

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/osipi/osipi.py
    ########################################################

    def _utcnow() -> datetime:
        return datetime.now(timezone.utc)


    def _isoformat_z(dt: datetime) -> str:
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


    def _parse_ts(value: str) -> datetime:
        return datetime.fromisoformat(value.replace("Z", "+00:00"))

    def _parse_pi_time(value: Optional[str], now: Optional[datetime] = None) -> datetime:
        """
        Parse PI Web API time expressions commonly used in query params.

        Supports:
        - "*" (now)
        - "*-10m", "*-2h", "*-7d" (relative to now)
        - ISO timestamps with or without Z suffix
        """
        now_dt = now or _utcnow()
        if value is None or value == "" or value == "*":
            return now_dt

        v = str(value).strip()
        if v.startswith("*-") and len(v) >= 4:
            num = v[2:-1]
            unit = v[-1]
            try:
                n = int(num)
                if unit == "m":
                    return now_dt - timedelta(minutes=n)
                if unit == "h":
                    return now_dt - timedelta(hours=n)
                if unit == "d":
                    return now_dt - timedelta(days=n)
            except Exception:
                pass

        try:
            dt = datetime.fromisoformat(v.replace("Z", "+00:00"))
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(timezone.utc)
        except Exception:
            return now_dt


    def _chunks(items: List[str], n: int) -> List[List[str]]:
        if n <= 0:
            return [items]
        return [items[i : i + n] for i in range(0, len(items), n)]



    def _as_bool(v: Any, default: bool = False) -> bool:
        if v is None:
            return default
        if isinstance(v, bool):
            return v
        if isinstance(v, (int, float)):
            return bool(v)
        s = str(v).strip().lower()
        if s in ("true", "t", "1", "yes", "y"):
            return True
        if s in ("false", "f", "0", "no", "n"):
            return False
        return default


    def _try_float(v: Any) -> Optional[float]:
        if v is None:
            return None
        try:
            return float(v)
        except Exception:
            return None


    def _batch_request_dict(requests_list: List[dict]) -> dict:
        """PI Web API docs define the batch request body as a dictionary keyed by ids."""
        return {str(i + 1): req for i, req in enumerate(requests_list)}


    def _batch_response_items(resp_json: dict) -> List[Tuple[str, dict]]:
        """Normalize PI Web API batch response into a list of (request_id, response_obj)."""
        if not isinstance(resp_json, dict):
            return []

        # Official PI Web API batch response is a dictionary keyed by request ids.
        # Some mocks use {"Responses": [...]}.
        if "Responses" in resp_json and isinstance(resp_json.get("Responses"), list):
            return [(str(i + 1), r) for i, r in enumerate(resp_json.get("Responses") or [])]

        # Otherwise treat top-level keys as request ids.
        out = []
        for k, v in resp_json.items():
            if isinstance(v, dict) and ("Status" in v or "Content" in v or "Headers" in v):
                out.append((str(k), v))
        # preserve numeric ordering when possible
        def keyfn(x):
            try:
                return int(x[0])
            except Exception:
                return 10**18
        return sorted(out, key=keyfn)


    class LakeflowConnect:
        TABLE_DATASERVERS = "pi_dataservers"
        TABLE_POINTS = "pi_points"
        TABLE_POINT_ATTRIBUTES = "pi_point_attributes"
        TABLE_TIMESERIES = "pi_timeseries"
        TABLE_INTERPOLATED = "pi_interpolated"
        TABLE_PLOT = "pi_plot"
        TABLE_AF_HIERARCHY = "pi_af_hierarchy"
        TABLE_EVENT_FRAMES = "pi_event_frames"

        TABLE_CURRENT_VALUE = "pi_current_value"
        TABLE_SUMMARY = "pi_summary"
        TABLE_STREAMSET_RECORDED = "pi_streamset_recorded"
        TABLE_STREAMSET_INTERPOLATED = "pi_streamset_interpolated"
        TABLE_STREAMSET_SUMMARY = "pi_streamset_summary"
        TABLE_ELEMENT_ATTRIBUTES = "pi_element_attributes"
        TABLE_EVENTFRAME_ATTRIBUTES = "pi_eventframe_attributes"
        TABLE_ASSET_SERVERS = "pi_assetservers"
        TABLE_ASSET_DATABASES = "pi_assetdatabases"
        TABLE_ELEMENT_TEMPLATES = "pi_element_templates"
        TABLE_CATEGORIES = "pi_categories"
        TABLE_ATTRIBUTE_TEMPLATES = "pi_attribute_templates"
        TABLE_ANALYSES = "pi_analyses"
        TABLE_EVENTFRAME_TEMPLATES = "pi_eventframe_templates"
        TABLE_END = "pi_end"
        TABLE_VALUE_AT_TIME = "pi_value_at_time"
        TABLE_STREAMSET_PLOT = "pi_streamset_plot"
        TABLE_UNITS_OF_MEASURE = "pi_units_of_measure"
        TABLE_ANALYSIS_TEMPLATES = "pi_analysis_templates"
        TABLE_EVENTFRAME_TEMPLATE_ATTRIBUTES = "pi_eventframe_template_attributes"
        TABLE_STREAMSET_END = "pi_streamset_end"
        TABLE_ELEMENT_TEMPLATE_ATTRIBUTES = "pi_element_template_attributes"
        TABLE_EVENTFRAME_REFERENCED_ELEMENTS = "pi_eventframe_referenced_elements"
        TABLE_AF_TABLES = "pi_af_tables"
        TABLE_AF_TABLE_ROWS = "pi_af_table_rows"
        TABLE_EVENTFRAME_ACKS = "pi_eventframe_acknowledgements"
        TABLE_EVENTFRAME_ANNOTATIONS = "pi_eventframe_annotations"
        TABLE_RECORDED_AT_TIME = "pi_recorded_at_time"
        TABLE_CALCULATED = "pi_calculated"
        TABLE_POINT_TYPE_CATALOG = "pi_point_type_catalog"
        TABLE_LINKS = "pi_links"
        TABLE_ERRORS = "pi_errors"

        def __init__(self, options: Dict[str, str]) -> None:
            self.options = options
            self.base_url = (options.get("pi_base_url") or options.get("pi_web_api_url") or "").rstrip("/")
            if not self.base_url:
                raise ValueError("Missing required option: pi_base_url (or pi_web_api_url)")

            self.session = requests.Session()
            self.session.headers.update({"Accept": "application/json"})
            self.verify_ssl = _as_bool(options.get("verify_ssl"), default=True)
            self._auth_resolved = False
            # In-memory diagnostics (best-effort). Exposed via table: pi_errors.
            self._errors: List[dict] = []

        def list_tables(self) -> List[str]:
            return [
                self.TABLE_DATASERVERS,
                self.TABLE_POINTS,
                self.TABLE_POINT_ATTRIBUTES,
                self.TABLE_TIMESERIES,
                self.TABLE_INTERPOLATED,
                self.TABLE_PLOT,
                self.TABLE_AF_HIERARCHY,
                self.TABLE_EVENT_FRAMES,
                self.TABLE_CURRENT_VALUE,
                self.TABLE_SUMMARY,
                self.TABLE_STREAMSET_RECORDED,
                self.TABLE_STREAMSET_INTERPOLATED,
                self.TABLE_STREAMSET_SUMMARY,
                self.TABLE_ELEMENT_ATTRIBUTES,
                self.TABLE_EVENTFRAME_ATTRIBUTES,
                self.TABLE_ASSET_SERVERS,
                self.TABLE_ASSET_DATABASES,
                self.TABLE_ELEMENT_TEMPLATES,
                self.TABLE_CATEGORIES,
                self.TABLE_ATTRIBUTE_TEMPLATES,
                self.TABLE_ANALYSES,
                self.TABLE_EVENTFRAME_TEMPLATES,
                self.TABLE_END,
                self.TABLE_VALUE_AT_TIME,
                self.TABLE_STREAMSET_PLOT,
                self.TABLE_UNITS_OF_MEASURE,
                self.TABLE_ANALYSIS_TEMPLATES,
                self.TABLE_EVENTFRAME_TEMPLATE_ATTRIBUTES,
                self.TABLE_STREAMSET_END,
                self.TABLE_ELEMENT_TEMPLATE_ATTRIBUTES,
                self.TABLE_EVENTFRAME_REFERENCED_ELEMENTS,
                self.TABLE_AF_TABLES,
                self.TABLE_AF_TABLE_ROWS,
                self.TABLE_EVENTFRAME_ACKS,
                self.TABLE_EVENTFRAME_ANNOTATIONS,
                self.TABLE_RECORDED_AT_TIME,
                self.TABLE_CALCULATED,
                self.TABLE_POINT_TYPE_CATALOG,
                self.TABLE_LINKS,
                self.TABLE_ERRORS,
            ]

        def get_table_schema(self, table_name: str, table_options: Dict[str, str]) -> StructType:
            if table_name == self.TABLE_DATASERVERS:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                ])

            if table_name == self.TABLE_POINTS:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("descriptor", StringType(), True),
                    StructField("engineering_units", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("dataserver_webid", StringType(), True),
                ])

            if table_name == self.TABLE_POINT_ATTRIBUTES:
                return StructType([
                    StructField("point_webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("value", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name in (
                self.TABLE_TIMESERIES,
                self.TABLE_STREAMSET_RECORDED,
                self.TABLE_INTERPOLATED,
                self.TABLE_STREAMSET_INTERPOLATED,
                self.TABLE_PLOT,
            ):
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("timestamp", TimestampType(), False),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_CURRENT_VALUE:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("timestamp", TimestampType(), True),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_SUMMARY:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("summary_type", StringType(), False),
                    StructField("timestamp", TimestampType(), True),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_STREAMSET_SUMMARY:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("summary_type", StringType(), False),
                    StructField("timestamp", TimestampType(), False),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_AF_HIERARCHY:
                return StructType([
                    StructField("element_webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("template_name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("parent_webid", StringType(), True),
                    StructField("depth", LongType(), True),
                    StructField("category_names", ArrayType(StringType()), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_EVENT_FRAMES:
                return StructType([
                    StructField("event_frame_webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("template_name", StringType(), True),
                    StructField("start_time", TimestampType(), True),
                    StructField("end_time", TimestampType(), True),
                    StructField("primary_referenced_element_webid", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("category_names", ArrayType(StringType()), True),
                    StructField("attributes", MapType(StringType(), StringType()), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_ELEMENT_ATTRIBUTES:
                return StructType([
                    StructField("element_webid", StringType(), False),
                    StructField("attribute_webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("default_units_name", StringType(), True),
                    StructField("data_reference_plugin", StringType(), True),
                    StructField("is_configuration_item", BooleanType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_EVENTFRAME_ATTRIBUTES:
                return StructType([
                    StructField("event_frame_webid", StringType(), False),
                    StructField("attribute_webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("default_units_name", StringType(), True),
                    StructField("data_reference_plugin", StringType(), True),
                    StructField("is_configuration_item", BooleanType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_ASSET_SERVERS:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("path", StringType(), True),
                ])

            if table_name == self.TABLE_ASSET_DATABASES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("assetserver_webid", StringType(), True),
                ])

            if table_name == self.TABLE_ELEMENT_TEMPLATES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_CATEGORIES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("category_type", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_ATTRIBUTE_TEMPLATES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("elementtemplate_webid", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_ANALYSES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("analysis_template_name", StringType(), True),
                    StructField("target_element_webid", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_EVENTFRAME_TEMPLATES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_END:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("timestamp", TimestampType(), True),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_VALUE_AT_TIME:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("timestamp", TimestampType(), True),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_STREAMSET_PLOT:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("timestamp", TimestampType(), False),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_UNITS_OF_MEASURE:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("abbreviation", StringType(), True),
                    StructField("quantity_type", StringType(), True),
                ])

            if table_name == self.TABLE_ANALYSIS_TEMPLATES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_EVENTFRAME_TEMPLATE_ATTRIBUTES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("eventframe_template_webid", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_STREAMSET_END:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("timestamp", TimestampType(), True),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_ELEMENT_TEMPLATE_ATTRIBUTES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("default_units_name", StringType(), True),
                    StructField("data_reference_plugin", StringType(), True),
                    StructField("is_configuration_item", BooleanType(), True),
                    StructField("elementtemplate_webid", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_EVENTFRAME_REFERENCED_ELEMENTS:
                return StructType([
                    StructField("event_frame_webid", StringType(), False),
                    StructField("element_webid", StringType(), False),
                    StructField("relationship_type", StringType(), True),
                    StructField("start_time", TimestampType(), True),
                    StructField("end_time", TimestampType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_AF_TABLES:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("assetdatabase_webid", StringType(), True),
                ])

            if table_name == self.TABLE_AF_TABLE_ROWS:
                return StructType([
                    StructField("table_webid", StringType(), False),
                    StructField("row_index", LongType(), True),
                    StructField("columns", MapType(StringType(), StringType()), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_EVENTFRAME_ACKS:
                return StructType([
                    StructField("event_frame_webid", StringType(), False),
                    StructField("ack_id", StringType(), False),
                    StructField("ack_timestamp", TimestampType(), True),
                    StructField("ack_user", StringType(), True),
                    StructField("comment", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_EVENTFRAME_ANNOTATIONS:
                return StructType([
                    StructField("event_frame_webid", StringType(), False),
                    StructField("annotation_id", StringType(), False),
                    StructField("annotation_timestamp", TimestampType(), True),
                    StructField("annotation_user", StringType(), True),
                    StructField("text", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_RECORDED_AT_TIME:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("query_time", TimestampType(), True),
                    StructField("timestamp", TimestampType(), True),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("questionable", BooleanType(), True),
                    StructField("substituted", BooleanType(), True),
                    StructField("annotated", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_CALCULATED:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("timestamp", TimestampType(), False),
                    StructField("value", DoubleType(), True),
                    StructField("units", StringType(), True),
                    StructField("calculation_type", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_POINT_TYPE_CATALOG:
                return StructType([
                    StructField("point_type", StringType(), False),
                    StructField("engineering_units", StringType(), True),
                    StructField("count_points", LongType(), True),
                ])

            if table_name == self.TABLE_LINKS:
                return StructType([
                    StructField("entity_type", StringType(), False),
                    StructField("webid", StringType(), False),
                    StructField("rel", StringType(), False),
                    StructField("href", StringType(), True),
                ])

            if table_name == self.TABLE_ERRORS:
                return StructType([
                    StructField("table_name", StringType(), False),
                    StructField("endpoint", StringType(), True),
                    StructField("status_code", LongType(), True),
                    StructField("error", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            raise ValueError(f"Unknown table: {table_name}")

        def read_table_metadata(self, table_name: str, table_options: Dict[str, str]) -> Dict:
            if table_name == self.TABLE_DATASERVERS:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_POINTS:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_POINT_ATTRIBUTES:
                return {"primary_keys": ["point_webid", "name"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name in (self.TABLE_TIMESERIES, self.TABLE_STREAMSET_RECORDED, self.TABLE_INTERPOLATED, self.TABLE_STREAMSET_INTERPOLATED, self.TABLE_PLOT):
                return {"primary_keys": ["tag_webid", "timestamp"], "cursor_field": "timestamp", "ingestion_type": "append"}
            if table_name == self.TABLE_CURRENT_VALUE:
                return {"primary_keys": ["tag_webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_SUMMARY:
                return {"primary_keys": ["tag_webid", "summary_type"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_AF_HIERARCHY:
                return {"primary_keys": ["element_webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_EVENT_FRAMES:
                return {"primary_keys": ["event_frame_webid", "start_time"], "cursor_field": "start_time", "ingestion_type": "append"}
            if table_name == self.TABLE_ELEMENT_ATTRIBUTES:
                return {"primary_keys": ["element_webid", "attribute_webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_EVENTFRAME_ATTRIBUTES:
                return {"primary_keys": ["event_frame_webid", "attribute_webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_STREAMSET_SUMMARY:
                return {"primary_keys": ["tag_webid", "summary_type", "timestamp"], "cursor_field": "timestamp", "ingestion_type": "append"}
            if table_name == self.TABLE_ASSET_SERVERS:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_ASSET_DATABASES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_ELEMENT_TEMPLATES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_CATEGORIES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_ATTRIBUTE_TEMPLATES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_ANALYSES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_EVENTFRAME_TEMPLATES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_END:
                return {"primary_keys": ["tag_webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_VALUE_AT_TIME:
                # snapshot at a requested time (optionally via table option `time`)
                return {"primary_keys": ["tag_webid", "timestamp"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_STREAMSET_PLOT:
                return {"primary_keys": ["tag_webid", "timestamp"], "cursor_field": "timestamp", "ingestion_type": "append"}
            if table_name == self.TABLE_UNITS_OF_MEASURE:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_ANALYSIS_TEMPLATES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_EVENTFRAME_TEMPLATE_ATTRIBUTES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_STREAMSET_END:
                return {"primary_keys": ["tag_webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_ELEMENT_TEMPLATE_ATTRIBUTES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_EVENTFRAME_REFERENCED_ELEMENTS:
                return {"primary_keys": ["event_frame_webid", "element_webid"], "cursor_field": "start_time", "ingestion_type": "append"}
            if table_name == self.TABLE_AF_TABLES:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_AF_TABLE_ROWS:
                return {"primary_keys": ["table_webid", "row_index"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_EVENTFRAME_ACKS:
                return {"primary_keys": ["event_frame_webid", "ack_id"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_EVENTFRAME_ANNOTATIONS:
                return {"primary_keys": ["event_frame_webid", "annotation_id"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_RECORDED_AT_TIME:
                return {"primary_keys": ["tag_webid", "query_time"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_CALCULATED:
                return {"primary_keys": ["tag_webid", "timestamp", "calculation_type"], "cursor_field": "timestamp", "ingestion_type": "append"}
            if table_name == self.TABLE_POINT_TYPE_CATALOG:
                return {"primary_keys": ["point_type", "engineering_units"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_LINKS:
                return {"primary_keys": ["entity_type", "webid", "rel"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_ERRORS:
                return {"primary_keys": ["table_name", "endpoint"], "cursor_field": None, "ingestion_type": "snapshot"}
            raise ValueError(f"Unknown table: {table_name}")

        def read_table(self, table_name: str, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            self._ensure_auth()

            if table_name == self.TABLE_DATASERVERS:
                return iter(self._read_dataservers()), {"offset": "done"}
            if table_name == self.TABLE_POINTS:
                return iter(self._read_points(table_options)), {"offset": "done"}
            if table_name == self.TABLE_POINT_ATTRIBUTES:
                return iter(self._read_point_attributes(table_options)), {"offset": "done"}
            if table_name == self.TABLE_TIMESERIES:
                return self._read_timeseries(start_offset, table_options)
            if table_name == self.TABLE_STREAMSET_RECORDED:
                return self._read_streamset_recorded(start_offset, table_options)
            if table_name == self.TABLE_INTERPOLATED:
                return self._read_interpolated(start_offset, table_options)
            if table_name == self.TABLE_STREAMSET_INTERPOLATED:
                return self._read_streamset_interpolated(start_offset, table_options)
            if table_name == self.TABLE_PLOT:
                return self._read_plot(start_offset, table_options)
            if table_name == self.TABLE_STREAMSET_SUMMARY:
                return self._read_streamset_summary(start_offset, table_options)
            if table_name == self.TABLE_CURRENT_VALUE:
                return iter(self._read_current_value(table_options)), {"offset": "done"}
            if table_name == self.TABLE_SUMMARY:
                return iter(self._read_summary(table_options)), {"offset": "done"}
            if table_name == self.TABLE_AF_HIERARCHY:
                return iter(self._read_af_hierarchy()), {"offset": "done"}
            if table_name == self.TABLE_EVENT_FRAMES:
                return self._read_event_frames(start_offset, table_options)
            if table_name == self.TABLE_ELEMENT_ATTRIBUTES:
                return iter(self._read_element_attributes(table_options)), {"offset": "done"}
            if table_name == self.TABLE_EVENTFRAME_ATTRIBUTES:
                return iter(self._read_eventframe_attributes(table_options)), {"offset": "done"}
            if table_name == self.TABLE_ASSET_SERVERS:
                return iter(self._read_assetservers_table()), {"offset": "done"}
            if table_name == self.TABLE_ASSET_DATABASES:
                return iter(self._read_assetdatabases_table()), {"offset": "done"}
            if table_name == self.TABLE_ELEMENT_TEMPLATES:
                return iter(self._read_element_templates_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_CATEGORIES:
                return iter(self._read_categories_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_ATTRIBUTE_TEMPLATES:
                return iter(self._read_attribute_templates_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_ANALYSES:
                return iter(self._read_analyses_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_EVENTFRAME_TEMPLATES:
                return iter(self._read_eventframe_templates_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_END:
                return iter(self._read_end(table_options)), {"offset": "done"}
            if table_name == self.TABLE_VALUE_AT_TIME:
                return iter(self._read_value_at_time(table_options)), {"offset": "done"}
            if table_name == self.TABLE_STREAMSET_PLOT:
                return self._read_streamset_plot(start_offset, table_options)
            if table_name == self.TABLE_UNITS_OF_MEASURE:
                return iter(self._read_units_of_measure_table()), {"offset": "done"}
            if table_name == self.TABLE_ANALYSIS_TEMPLATES:
                return iter(self._read_analysis_templates_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_EVENTFRAME_TEMPLATE_ATTRIBUTES:
                return iter(self._read_eventframe_template_attributes_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_STREAMSET_END:
                return iter(self._read_streamset_end(table_options)), {"offset": "done"}
            if table_name == self.TABLE_ELEMENT_TEMPLATE_ATTRIBUTES:
                return iter(self._read_element_template_attributes_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_EVENTFRAME_REFERENCED_ELEMENTS:
                return self._read_eventframe_referenced_elements(start_offset, table_options)
            if table_name == self.TABLE_AF_TABLES:
                return iter(self._read_af_tables_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_AF_TABLE_ROWS:
                return iter(self._read_af_table_rows_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_EVENTFRAME_ACKS:
                return iter(self._read_eventframe_acknowledgements_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_EVENTFRAME_ANNOTATIONS:
                return iter(self._read_eventframe_annotations_table(table_options)), {"offset": "done"}
            if table_name == self.TABLE_RECORDED_AT_TIME:
                return iter(self._read_recorded_at_time(table_options)), {"offset": "done"}
            if table_name == self.TABLE_CALCULATED:
                return self._read_calculated(start_offset, table_options)
            if table_name == self.TABLE_POINT_TYPE_CATALOG:
                return iter(self._read_point_type_catalog(table_options)), {"offset": "done"}
            if table_name == self.TABLE_LINKS:
                return iter(self._read_links(table_options)), {"offset": "done"}
            if table_name == self.TABLE_ERRORS:
                return iter(self._read_errors(table_options)), {"offset": "done"}

            raise ValueError(f"Unknown table: {table_name}")

        def _ensure_auth(self) -> None:
            if self._auth_resolved:
                return

            access_token = self.options.get("access_token")
            if access_token:
                self.session.headers.update({"Authorization": f"Bearer {access_token}"})
                self._auth_resolved = True
                return

            workspace_host = (self.options.get("workspace_host") or "").rstrip("/")
            client_id = self.options.get("client_id")
            client_secret = self.options.get("client_secret")
            if workspace_host and client_id and client_secret:
                if not workspace_host.startswith("http://") and not workspace_host.startswith("https://"):
                    workspace_host = "https://" + workspace_host

                token_url = f"{workspace_host}/oidc/v1/token"
                resp = requests.post(
                    token_url,
                    data={"grant_type": "client_credentials", "scope": "all-apis"},
                    auth=(client_id, client_secret),
                    headers={"Content-Type": "application/x-www-form-urlencoded"},
                    timeout=30,
                )
                resp.raise_for_status()
                token = resp.json().get("access_token")
                if not token:
                    raise RuntimeError("OIDC token endpoint did not return access_token")
                self.session.headers.update({"Authorization": f"Bearer {token}"})
                self._auth_resolved = True
                return

            username = self.options.get("username")
            password = self.options.get("password")
            if username and password:
                self.session.auth = (username, password)
                self._auth_resolved = True
                return

            self._auth_resolved = True

        def _get_json(self, path: str, params: Optional[Any] = None) -> dict:
            url = f"{self.base_url}{path}"
            r = self.session.get(url, params=params, timeout=60, verify=self.verify_ssl)
            r.raise_for_status()
            return r.json()

        def _post_json(self, path: str, payload: Any) -> dict:
            url = f"{self.base_url}{path}"
            r = self.session.post(url, json=payload, timeout=120, verify=self.verify_ssl)
            r.raise_for_status()
            return r.json()

        def _record_error(self, table_name: str, endpoint: str, error: Exception) -> None:
            try:
                status_code = None
                if isinstance(error, requests.exceptions.HTTPError) and getattr(error, "response", None) is not None:
                    status_code = getattr(error.response, "status_code", None)
                self._errors.append(
                    {
                        "table_name": table_name,
                        "endpoint": endpoint,
                        "status_code": status_code,
                        "error": str(error),
                        "ingestion_timestamp": _utcnow(),
                    }
                )
                # cap memory
                if len(self._errors) > 200:
                    self._errors = self._errors[-200:]
            except Exception:
                # never fail ingestion due to diagnostics bookkeeping
                return

        def _batch_execute(self, requests_list: List[dict]) -> List[Tuple[str, dict]]:
            payload = _batch_request_dict(requests_list)
            resp_json = self._post_json("/piwebapi/batch", payload)
            return _batch_response_items(resp_json)

        def _read_dataservers(self) -> List[dict]:
            data = self._get_json("/piwebapi/dataservers")
            items = data.get("Items", []) or []
            return [{"webid": i.get("WebId"), "name": i.get("Name")} for i in items if i.get("WebId")]

        def _read_points(self, table_options: Dict[str, str]) -> List[dict]:
            dataservers = self._read_dataservers()
            if not dataservers:
                return []
            server_webid = table_options.get("dataserver_webid") or dataservers[0]["webid"]

            page_size = int(table_options.get("maxCount", 1000))
            start_index = int(table_options.get("startIndex", 0))
            max_total = int(table_options.get("maxTotalCount", 100000))
            name_filter = table_options.get("nameFilter")

            out: List[dict] = []
            while start_index < max_total:
                params: Dict[str, str] = {"maxCount": str(page_size), "startIndex": str(start_index)}
                if name_filter:
                    params["nameFilter"] = str(name_filter)

                data = self._get_json(f"/piwebapi/dataservers/{server_webid}/points", params=params)
                items = data.get("Items", []) or []

                for p in items:
                    out.append({
                        "webid": p.get("WebId"),
                        "name": p.get("Name"),
                        "descriptor": p.get("Descriptor", ""),
                        "engineering_units": p.get("EngineeringUnits", ""),
                        "path": p.get("Path", ""),
                        "dataserver_webid": server_webid,
                    })

                if len(items) < page_size:
                    break
                start_index += page_size

            return [r for r in out if r.get("webid")]

        def _resolve_tag_webids(self, table_options: Dict[str, str]) -> List[str]:
            tag_webids_csv = table_options.get("tag_webids") or self.options.get("tag_webids") or ""
            tag_webids = [t.strip() for t in str(tag_webids_csv).split(",") if t.strip()]
            if tag_webids:
                return tag_webids
            pts = self._read_points(table_options)
            return [p["webid"] for p in pts[: int(table_options.get("default_tags", 50))]]

        def _read_point_attributes(self, table_options: Dict[str, str]) -> List[dict]:
            # Options:
            # - point_webids: csv of point WebIds
            # - default_points: sample size if point_webids not provided
            point_webids_csv = (table_options.get("point_webids") or self.options.get("point_webids") or "").strip()
            point_webids = [t.strip() for t in point_webids_csv.split(",") if t.strip()]
            if not point_webids:
                default_points = int(table_options.get("default_points", 10))
                pts = self._read_points(table_options)
                point_webids = [p["webid"] for p in pts[:default_points] if p.get("webid")]

            params: Dict[str, str] = {}
            selected_fields = table_options.get("selectedFields")
            if selected_fields:
                params["selectedFields"] = selected_fields

            out: List[dict] = []
            ingest_ts = _utcnow()

            for wid in point_webids:
                try:
                    data = self._get_json(f"/piwebapi/points/{wid}/attributes", params=params or None)
                    for item in (data.get("Items") or []):
                        out.append({
                            "point_webid": wid,
                            "name": item.get("Name"),
                            "value": None if item.get("Value") is None else str(item.get("Value")),
                            "type": item.get("Type") or item.get("ValueType") or "",
                            "ingestion_timestamp": ingest_ts,
                        })
                except Exception:
                    continue

            return out


        def _read_timeseries(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            tag_webids = self._resolve_tag_webids(table_options)
            now = _utcnow()

            # Time range controls
            end_opt = table_options.get("endTime") or table_options.get("end_time") or "*"
            end_dt = _parse_pi_time(end_opt, now=now)

            start_dt: Optional[datetime] = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start_dt = _parse_ts(off)
                    except Exception:
                        start_dt = None

            if start_dt is None:
                start_opt = table_options.get("startTime") or table_options.get("start_time")
                if start_opt:
                    start_dt = _parse_pi_time(str(start_opt), now=end_dt)
                else:
                    lookback_minutes = int(table_options.get("lookback_minutes", 60))
                    start_dt = end_dt - timedelta(minutes=lookback_minutes)

            window_seconds = int(table_options.get("window_seconds", 0) or 0)
            end_window = min(end_dt, start_dt + timedelta(seconds=window_seconds)) if window_seconds > 0 else end_dt

            start_str = _isoformat_z(start_dt)
            end_str = _isoformat_z(end_window)
            max_count = int(table_options.get("maxCount", 1000))
            ingest_ts = _utcnow()

            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]

            prefer_streamset = _as_bool(table_options.get("prefer_streamset", True), default=True)
            selected_fields = table_options.get("selectedFields")

            next_offset = {"offset": end_str}

            # Preferred: StreamSet GetRecordedAdHoc for multi-tag reads.
            if prefer_streamset and len(tag_webids) > 1:
                def iterator() -> Iterator[dict]:
                    for group in groups:
                        if not group:
                            continue
                        params: List[Tuple[str, str]] = [("webId", w) for w in group]
                        params += [("startTime", start_str), ("endTime", end_str), ("maxCount", str(max_count))]
                        if selected_fields:
                            params.append(("selectedFields", str(selected_fields)))
                        data = self._get_json("/piwebapi/streamsets/recorded", params=params)
                        for stream in data.get("Items", []) or []:
                            webid = stream.get("WebId")
                            if not webid:
                                continue
                            for item in stream.get("Items", []) or []:
                                ts = item.get("Timestamp")
                                if not ts:
                                    continue
                                yield {
                                    "tag_webid": webid,
                                    "timestamp": _parse_ts(ts),
                                    "value": _try_float(item.get("Value")),
                                    "good": _as_bool(item.get("Good"), default=True),
                                    "questionable": _as_bool(item.get("Questionable"), default=False),
                                    "substituted": _as_bool(item.get("Substituted"), default=False),
                                    "annotated": _as_bool(item.get("Annotated"), default=False),
                                    "units": item.get("UnitsAbbreviation", ""),
                                    "ingestion_timestamp": ingest_ts,
                                }

                return iterator(), next_offset

            # Fallback: Batch execute many Stream GetRecorded calls (chunked).
            def iterator() -> Iterator[dict]:
                for group in groups:
                    if not group:
                        continue
                    reqs = [
                        {
                            "Method": "GET",
                            "Resource": f"/piwebapi/streams/{webid}/recorded",
                            "Parameters": {"startTime": start_str, "endTime": end_str, "maxCount": str(max_count)},
                        }
                        for webid in group
                    ]
                    responses = self._batch_execute(reqs)
                    for idx, (_rid, resp) in enumerate(responses):
                        if resp.get("Status") != 200:
                            continue
                        webid = group[idx] if idx < len(group) else None
                        if not webid:
                            continue
                        content = resp.get("Content", {}) or {}
                        for item in content.get("Items", []) or []:
                            ts = item.get("Timestamp")
                            if not ts:
                                continue
                            yield {
                                "tag_webid": webid,
                                "timestamp": _parse_ts(ts),
                                "value": _try_float(item.get("Value")),
                                "good": _as_bool(item.get("Good"), default=True),
                                "questionable": _as_bool(item.get("Questionable"), default=False),
                                "substituted": _as_bool(item.get("Substituted"), default=False),
                                "annotated": _as_bool(item.get("Annotated"), default=False),
                                "units": item.get("UnitsAbbreviation", ""),
                                "ingestion_timestamp": ingest_ts,
                            }

            return iterator(), next_offset

        def _read_streamset_recorded(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            # Explicit StreamSet recorded table (same output schema as pi_timeseries)
            tag_webids = self._resolve_tag_webids(table_options)
            now = _utcnow()

            end_opt = table_options.get("endTime") or table_options.get("end_time") or "*"
            end_dt = _parse_pi_time(end_opt, now=now)

            start_dt: Optional[datetime] = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start_dt = _parse_ts(off)
                    except Exception:
                        start_dt = None

            if start_dt is None:
                start_opt = table_options.get("startTime") or table_options.get("start_time")
                if start_opt:
                    start_dt = _parse_pi_time(str(start_opt), now=end_dt)
                else:
                    lookback_minutes = int(table_options.get("lookback_minutes", 60))
                    start_dt = end_dt - timedelta(minutes=lookback_minutes)

            window_seconds = int(table_options.get("window_seconds", 0) or 0)
            end_window = min(end_dt, start_dt + timedelta(seconds=window_seconds)) if window_seconds > 0 else end_dt

            start_str = _isoformat_z(start_dt)
            end_str = _isoformat_z(end_window)
            max_count = int(table_options.get("maxCount", 1000))
            ingest_ts = _utcnow()

            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]
            selected_fields = table_options.get("selectedFields")

            def iterator() -> Iterator[dict]:
                for group in groups:
                    if not group:
                        continue
                    params: List[Tuple[str, str]] = [("webId", w) for w in group]
                    params += [("startTime", start_str), ("endTime", end_str), ("maxCount", str(max_count))]
                    if selected_fields:
                        params.append(("selectedFields", str(selected_fields)))
                    data = self._get_json("/piwebapi/streamsets/recorded", params=params)
                    for stream in data.get("Items", []) or []:
                        webid = stream.get("WebId")
                        if not webid:
                            continue
                        for item in stream.get("Items", []) or []:
                            ts = item.get("Timestamp")
                            if not ts:
                                continue
                            yield {
                                "tag_webid": webid,
                                "timestamp": _parse_ts(ts),
                                "value": _try_float(item.get("Value")),
                                "good": _as_bool(item.get("Good"), default=True),
                                "questionable": _as_bool(item.get("Questionable"), default=False),
                                "substituted": _as_bool(item.get("Substituted"), default=False),
                                "annotated": _as_bool(item.get("Annotated"), default=False),
                                "units": item.get("UnitsAbbreviation", ""),
                                "ingestion_timestamp": ingest_ts,
                            }

            return iterator(), {"offset": end_str}



        def _read_interpolated(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            """Interpolated values for tags over a window (best-effort).

            If interpolated endpoints are not available on the PI Web API host, this returns an empty iterator.
            """
            tag_webids = self._resolve_tag_webids(table_options)
            now = _utcnow()

            end_opt = table_options.get("endTime") or table_options.get("end_time") or "*"
            end_dt = _parse_pi_time(end_opt, now=now)

            start_dt: Optional[datetime] = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start_dt = _parse_ts(off)
                    except Exception:
                        start_dt = None

            if start_dt is None:
                start_opt = table_options.get("startTime") or table_options.get("start_time")
                if start_opt:
                    start_dt = _parse_pi_time(str(start_opt), now=end_dt)
                else:
                    lookback_minutes = int(table_options.get("lookback_minutes", 60))
                    start_dt = end_dt - timedelta(minutes=lookback_minutes)

            start_str = _isoformat_z(start_dt)
            end_str = _isoformat_z(end_dt)

            interval = (table_options.get("interval") or table_options.get("sampleInterval") or "1m").strip()
            max_count = int(table_options.get("maxCount", 1000))
            ingest_ts = _utcnow()

            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]
            selected_fields = table_options.get("selectedFields")

            next_offset = {"offset": end_str}

            def emit_items(wid: str, items: List[dict]) -> Iterator[dict]:
                for item in items or []:
                    ts = item.get("Timestamp")
                    if not ts:
                        continue
                    yield {
                        "tag_webid": wid,
                        "timestamp": _parse_ts(ts),
                        "value": _try_float(item.get("Value")),
                        "good": _as_bool(item.get("Good"), default=True),
                        "questionable": _as_bool(item.get("Questionable"), default=False),
                        "substituted": _as_bool(item.get("Substituted"), default=False),
                        "annotated": _as_bool(item.get("Annotated"), default=False),
                        "units": item.get("UnitsAbbreviation", ""),
                        "ingestion_timestamp": ingest_ts,
                    }

            # Try StreamSet interpolated first for multi-tag reads.
            def iterator() -> Iterator[dict]:
                for group in groups:
                    if not group:
                        continue
                    # Prefer streamsets/interpolated when multiple tags
                    if len(group) > 1:
                        params: List[Tuple[str, str]] = [("webId", w) for w in group]
                        params += [("startTime", start_str), ("endTime", end_str), ("interval", interval), ("maxCount", str(max_count))]
                        if selected_fields:
                            params.append(("selectedFields", str(selected_fields)))
                        try:
                            data = self._get_json("/piwebapi/streamsets/interpolated", params=params)
                        except requests.exceptions.HTTPError as e:
                            if getattr(e.response, 'status_code', None) == 404:
                                data = None
                            else:
                                raise
                        if data:
                            for stream in data.get("Items", []) or []:
                                wid = stream.get("WebId")
                                if not wid:
                                    continue
                                yield from emit_items(wid, stream.get("Items", []) or [])
                            continue

                    # Fallback: per-tag interpolated via batch
                    reqs = [
                        {
                            "Method": "GET",
                            "Resource": f"/piwebapi/streams/{wid}/interpolated",
                            "Parameters": {"startTime": start_str, "endTime": end_str, "interval": interval, "maxCount": str(max_count)},
                        }
                        for wid in group
                    ]
                    try:
                        responses = self._batch_execute(reqs)
                    except requests.exceptions.HTTPError as e:
                        if getattr(e.response, 'status_code', None) == 404:
                            return
                        raise
                    for idx, (_rid, resp) in enumerate(responses):
                        if resp.get("Status") != 200:
                            continue
                        wid = group[idx] if idx < len(group) else None
                        if not wid:
                            continue
                        content = resp.get("Content", {}) or {}
                        yield from emit_items(wid, content.get("Items", []) or [])

            return iterator(), next_offset


        def _read_streamset_interpolated(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            """Explicit StreamSet interpolated table (same output schema as pi_interpolated)."""
            # Force streamset path by grouping all tags together.
            opts = dict(table_options)
            opts["tags_per_request"] = str(max(1, int(opts.get("tags_per_request", 0) or 0)))
            return self._read_interpolated(start_offset, opts)


        def _read_plot(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            """Plot values for tags over a window (best-effort).

            If plot endpoints are not available on the PI Web API host, this returns an empty iterator.
            """
            tag_webids = self._resolve_tag_webids(table_options)
            now = _utcnow()

            end_opt = table_options.get("endTime") or table_options.get("end_time") or "*"
            end_dt = _parse_pi_time(end_opt, now=now)

            start_dt: Optional[datetime] = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start_dt = _parse_ts(off)
                    except Exception:
                        start_dt = None

            if start_dt is None:
                start_opt = table_options.get("startTime") or table_options.get("start_time")
                if start_opt:
                    start_dt = _parse_pi_time(str(start_opt), now=end_dt)
                else:
                    lookback_minutes = int(table_options.get("lookback_minutes", 60))
                    start_dt = end_dt - timedelta(minutes=lookback_minutes)

            start_str = _isoformat_z(start_dt)
            end_str = _isoformat_z(end_dt)
            intervals = int(table_options.get("intervals", 300) or 300)
            ingest_ts = _utcnow()

            def iterator() -> Iterator[dict]:
                for wid in tag_webids:
                    try:
                        data = self._get_json(
                            f"/piwebapi/streams/{wid}/plot",
                            params={"startTime": start_str, "endTime": end_str, "intervals": str(intervals)},
                        )
                    except requests.exceptions.HTTPError as e:
                        if getattr(e.response, 'status_code', None) == 404:
                            return
                        raise

                    for item in (data.get("Items") or []):
                        ts = item.get("Timestamp")
                        if not ts:
                            continue
                        yield {
                            "tag_webid": wid,
                            "timestamp": _parse_ts(ts),
                            "value": _try_float(item.get("Value")),
                            "good": _as_bool(item.get("Good"), default=True),
                            "questionable": _as_bool(item.get("Questionable"), default=False),
                            "substituted": _as_bool(item.get("Substituted"), default=False),
                            "annotated": _as_bool(item.get("Annotated"), default=False),
                            "units": item.get("UnitsAbbreviation", ""),
                            "ingestion_timestamp": ingest_ts,
                        }

            return iterator(), {"offset": end_str}


        def _read_streamset_summary(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            """Multi-tag summary (best-effort).

            If streamset summary endpoints are not available on the PI Web API host, this returns an empty iterator.
            """
            tag_webids = self._resolve_tag_webids(table_options)
            now = _utcnow()

            end_opt = table_options.get("endTime") or table_options.get("end_time") or "*"
            end_dt = _parse_pi_time(end_opt, now=now)

            start_dt: Optional[datetime] = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start_dt = _parse_ts(off)
                    except Exception:
                        start_dt = None

            if start_dt is None:
                start_opt = table_options.get("startTime") or table_options.get("start_time")
                if start_opt:
                    start_dt = _parse_pi_time(str(start_opt), now=end_dt)
                else:
                    lookback_minutes = int(table_options.get("lookback_minutes", 60))
                    start_dt = end_dt - timedelta(minutes=lookback_minutes)

            start_str = _isoformat_z(start_dt)
            end_str = _isoformat_z(end_dt)

            summary_type = (table_options.get("summaryType") or "Total").strip()
            calculation_basis = (table_options.get("calculationBasis") or "TimeWeighted").strip()
            summary_duration = (table_options.get("summaryDuration") or "1h").strip()
            selected_fields = table_options.get("selectedFields")

            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]

            ingest_ts = _utcnow()

            def iterator() -> Iterator[dict]:
                for group in groups:
                    if not group:
                        continue
                    params: List[Tuple[str, str]] = [("webId", w) for w in group]
                    params += [
                        ("startTime", start_str),
                        ("endTime", end_str),
                        ("summaryType", summary_type),
                        ("calculationBasis", calculation_basis),
                        ("summaryDuration", summary_duration),
                    ]
                    if selected_fields:
                        params.append(("selectedFields", str(selected_fields)))

                    try:
                        data = self._get_json("/piwebapi/streamsets/summary", params=params)
                    except requests.exceptions.HTTPError as e:
                        if getattr(e.response, 'status_code', None) == 404:
                            return
                        raise

                    for stream in data.get("Items", []) or []:
                        wid = stream.get("WebId")
                        if not wid:
                            continue
                        for item in stream.get("Items", []) or []:
                            stype = item.get("Type")
                            v = item.get("Value", {}) or {}
                            ts = v.get("Timestamp")
                            if not ts:
                                continue
                            yield {
                                "tag_webid": wid,
                                "summary_type": str(stype),
                                "timestamp": _parse_ts(ts),
                                "value": _try_float(v.get("Value")),
                                "good": _as_bool(v.get("Good"), default=True),
                                "questionable": _as_bool(v.get("Questionable"), default=False),
                                "substituted": _as_bool(v.get("Substituted"), default=False),
                                "annotated": _as_bool(v.get("Annotated"), default=False),
                                "units": v.get("UnitsAbbreviation", ""),
                                "ingestion_timestamp": ingest_ts,
                            }

            return iterator(), {"offset": end_str}


        def _read_assetservers_table(self) -> List[dict]:
            try:
                items = self._read_assetservers()
            except requests.exceptions.HTTPError as e:
                if getattr(e.response, 'status_code', None) == 404:
                    return []
                raise
            out: List[dict] = []
            for s in items or []:
                wid = s.get("WebId")
                if not wid:
                    continue
                out.append({"webid": wid, "name": s.get("Name", ""), "path": s.get("Path", "")})
            return out


        def _read_assetdatabases_table(self) -> List[dict]:
            try:
                assetservers = self._read_assetservers()
            except requests.exceptions.HTTPError as e:
                if getattr(e.response, 'status_code', None) == 404:
                    return []
                raise

            out: List[dict] = []
            for srv in assetservers or []:
                srv_wid = srv.get("WebId")
                if not srv_wid:
                    continue
                try:
                    dbs = self._read_assetdatabases(srv_wid)
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, 'status_code', None) == 404:
                        continue
                    raise
                for db in dbs or []:
                    db_wid = db.get("WebId")
                    if not db_wid:
                        continue
                    out.append({
                        "webid": db_wid,
                        "name": db.get("Name", ""),
                        "path": db.get("Path", ""),
                        "assetserver_webid": srv_wid,
                    })
            return out


        def _read_element_templates_table(self, table_options: Dict[str, str]) -> List[dict]:
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()
            if db_wid_opt:
                db_wids = [db_wid_opt]
            else:
                db_wids = [d.get("webid") for d in self._read_assetdatabases_table() if d.get("webid")]

            for db_wid in db_wids:
                try:
                    data = self._get_json(f"/piwebapi/assetdatabases/{db_wid}/elementtemplates")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, 'status_code', None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append({
                        "webid": wid,
                        "name": it.get("Name", ""),
                        "description": it.get("Description", ""),
                        "path": it.get("Path", ""),
                        "assetdatabase_webid": db_wid,
                    })
            return out


        def _read_categories_table(self, table_options: Dict[str, str]) -> List[dict]:
            """List AF categories (best-effort)."""
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()
            if db_wid_opt:
                db_wids = [db_wid_opt]
            else:
                db_wids = [d.get("webid") for d in self._read_assetdatabases_table() if d.get("webid")]

            for db_wid in db_wids:
                try:
                    data = self._get_json(f"/piwebapi/assetdatabases/{db_wid}/categories")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append(
                        {
                            "webid": wid,
                            "name": it.get("Name", ""),
                            "description": it.get("Description", ""),
                            "category_type": it.get("CategoryType", "") or it.get("Type", ""),
                            "assetdatabase_webid": db_wid,
                        }
                    )
            return out


        def _read_attribute_templates_table(self, table_options: Dict[str, str]) -> List[dict]:
            """List AF attribute templates for element templates (best-effort)."""
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()

            templates = self._read_element_templates_table({"assetdatabase_webid": db_wid_opt} if db_wid_opt else {})
            for tpl in templates:
                tpl_wid = tpl.get("webid")
                if not tpl_wid:
                    continue
                db_wid = tpl.get("assetdatabase_webid") or db_wid_opt or ""
                try:
                    data = self._get_json(f"/piwebapi/elementtemplates/{tpl_wid}/attributetemplates")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append(
                        {
                            "webid": wid,
                            "name": it.get("Name", ""),
                            "description": it.get("Description", ""),
                            "path": it.get("Path", ""),
                            "type": it.get("Type", "") or it.get("AttributeType", ""),
                            "elementtemplate_webid": tpl_wid,
                            "assetdatabase_webid": db_wid,
                        }
                    )
            return out


        def _read_analyses_table(self, table_options: Dict[str, str]) -> List[dict]:
            """List AF analyses (best-effort)."""
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()
            if db_wid_opt:
                db_wids = [db_wid_opt]
            else:
                db_wids = [d.get("webid") for d in self._read_assetdatabases_table() if d.get("webid")]

            for db_wid in db_wids:
                try:
                    data = self._get_json(f"/piwebapi/assetdatabases/{db_wid}/analyses")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append(
                        {
                            "webid": wid,
                            "name": it.get("Name", ""),
                            "description": it.get("Description", ""),
                            "path": it.get("Path", ""),
                            "analysis_template_name": it.get("AnalysisTemplateName", "") or it.get("TemplateName", ""),
                            "target_element_webid": it.get("TargetElementWebId", "") or it.get("TargetElement", ""),
                            "assetdatabase_webid": db_wid,
                        }
                    )
            return out


        def _read_eventframe_templates_table(self, table_options: Dict[str, str]) -> List[dict]:
            """List Event Frame templates (best-effort)."""
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()
            if db_wid_opt:
                db_wids = [db_wid_opt]
            else:
                db_wids = [d.get("webid") for d in self._read_assetdatabases_table() if d.get("webid")]

            for db_wid in db_wids:
                try:
                    data = self._get_json(f"/piwebapi/assetdatabases/{db_wid}/eventframetemplates")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append(
                        {
                            "webid": wid,
                            "name": it.get("Name", ""),
                            "description": it.get("Description", ""),
                            "path": it.get("Path", ""),
                            "assetdatabase_webid": db_wid,
                        }
                    )
            return out


        def _read_end(self, table_options: Dict[str, str]) -> List[dict]:
            """Stream GetEnd (best-effort)."""
            tag_webids = self._resolve_tag_webids(table_options)
            ingest_ts = _utcnow()
            out: List[dict] = []
            for wid in tag_webids:
                try:
                    v = self._get_json(f"/piwebapi/streams/{wid}/end")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                ts = v.get("Timestamp")
                out.append(
                    {
                        "tag_webid": wid,
                        "timestamp": _parse_ts(ts) if ts else None,
                        "value": _try_float(v.get("Value")),
                        "good": _as_bool(v.get("Good"), default=True),
                        "questionable": _as_bool(v.get("Questionable"), default=False),
                        "substituted": _as_bool(v.get("Substituted"), default=False),
                        "annotated": _as_bool(v.get("Annotated"), default=False),
                        "units": v.get("UnitsAbbreviation", ""),
                        "ingestion_timestamp": ingest_ts,
                    }
                )
            return out


        def _read_value_at_time(self, table_options: Dict[str, str]) -> List[dict]:
            """Stream GetValue at requested time (option `time`, defaults to '*')."""
            tag_webids = self._resolve_tag_webids(table_options)
            time_param = table_options.get("time") or "*"
            ingest_ts = _utcnow()
            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]

            out: List[dict] = []
            for group in groups:
                if not group:
                    continue
                reqs: List[dict] = [
                    {"Method": "GET", "Resource": f"/piwebapi/streams/{w}/value", "Parameters": {"time": str(time_param)}}
                    for w in group
                ]
                responses = self._batch_execute(reqs)
                for idx, (_rid, resp) in enumerate(responses):
                    if resp.get("Status") != 200:
                        continue
                    wid = group[idx] if idx < len(group) else None
                    if not wid:
                        continue
                    v = resp.get("Content", {}) or {}
                    ts = v.get("Timestamp")
                    out.append(
                        {
                            "tag_webid": wid,
                            "timestamp": _parse_ts(ts) if ts else None,
                            "value": _try_float(v.get("Value")),
                            "good": _as_bool(v.get("Good"), default=True),
                            "questionable": _as_bool(v.get("Questionable"), default=False),
                            "substituted": _as_bool(v.get("Substituted"), default=False),
                            "annotated": _as_bool(v.get("Annotated"), default=False),
                            "units": v.get("UnitsAbbreviation", ""),
                            "ingestion_timestamp": ingest_ts,
                        }
                    )
            return out


        def _read_streamset_plot(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            """StreamSet plot (best-effort): downsampled multi-tag points for visualization."""
            tag_webids = self._resolve_tag_webids(table_options)
            now = _utcnow()

            end_opt = table_options.get("endTime") or table_options.get("end_time") or "*"
            end_dt = _parse_pi_time(end_opt, now=now)

            start_dt: Optional[datetime] = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start_dt = _parse_ts(off)
                    except Exception:
                        start_dt = None

            if start_dt is None:
                start_opt = table_options.get("startTime") or table_options.get("start_time")
                if start_opt:
                    start_dt = _parse_pi_time(str(start_opt), now=end_dt)
                else:
                    lookback_minutes = int(table_options.get("lookback_minutes", 60))
                    start_dt = end_dt - timedelta(minutes=lookback_minutes)

            start_str = _isoformat_z(start_dt)
            end_str = _isoformat_z(end_dt)
            intervals = int(table_options.get("intervals", 300) or 300)
            ingest_ts = _utcnow()
            next_offset = {"offset": end_str}

            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]

            def iterator() -> Iterator[dict]:
                for group in groups:
                    if not group:
                        continue
                    params: List[Tuple[str, str]] = [("webId", w) for w in group]
                    params += [("startTime", start_str), ("endTime", end_str), ("intervals", str(intervals))]
                    try:
                        data = self._get_json("/piwebapi/streamsets/plot", params=params)
                    except requests.exceptions.HTTPError as e:
                        if getattr(e.response, "status_code", None) == 404:
                            data = None
                        else:
                            raise

                    if data:
                        for stream in data.get("Items", []) or []:
                            wid = stream.get("WebId")
                            if not wid:
                                continue
                            for item in stream.get("Items", []) or []:
                                ts = item.get("Timestamp")
                                if not ts:
                                    continue
                                yield {
                                    "tag_webid": wid,
                                    "timestamp": _parse_ts(ts),
                                    "value": _try_float(item.get("Value")),
                                    "good": _as_bool(item.get("Good"), default=True),
                                    "questionable": _as_bool(item.get("Questionable"), default=False),
                                    "substituted": _as_bool(item.get("Substituted"), default=False),
                                    "annotated": _as_bool(item.get("Annotated"), default=False),
                                    "units": item.get("UnitsAbbreviation", ""),
                                    "ingestion_timestamp": ingest_ts,
                                }
                        continue

                    for wid in group:
                        try:
                            pdata = self._get_json(
                                f"/piwebapi/streams/{wid}/plot",
                                params={"startTime": start_str, "endTime": end_str, "intervals": str(intervals)},
                            )
                        except requests.exceptions.HTTPError as e:
                            if getattr(e.response, "status_code", None) == 404:
                                continue
                            raise
                        for item in pdata.get("Items", []) or []:
                            ts = item.get("Timestamp")
                            if not ts:
                                continue
                            yield {
                                "tag_webid": wid,
                                "timestamp": _parse_ts(ts),
                                "value": _try_float(item.get("Value")),
                                "good": _as_bool(item.get("Good"), default=True),
                                "questionable": _as_bool(item.get("Questionable"), default=False),
                                "substituted": _as_bool(item.get("Substituted"), default=False),
                                "annotated": _as_bool(item.get("Annotated"), default=False),
                                "units": item.get("UnitsAbbreviation", ""),
                                "ingestion_timestamp": ingest_ts,
                            }

            return iterator(), next_offset


        def _read_units_of_measure_table(self) -> List[dict]:
            """List Units Of Measure (best-effort)."""
            try:
                data = self._get_json("/piwebapi/uoms")
            except requests.exceptions.HTTPError as e:
                if getattr(e.response, "status_code", None) == 404:
                    return []
                raise
            out: List[dict] = []
            for it in (data.get("Items") or []):
                wid = it.get("WebId")
                if not wid:
                    continue
                out.append(
                    {
                        "webid": wid,
                        "name": it.get("Name", ""),
                        "abbreviation": it.get("Abbreviation", "") or it.get("Symbol", ""),
                        "quantity_type": it.get("QuantityType", "") or it.get("Quantity", ""),
                    }
                )
            return out


        def _read_analysis_templates_table(self, table_options: Dict[str, str]) -> List[dict]:
            """List analysis templates (best-effort)."""
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()
            if db_wid_opt:
                db_wids = [db_wid_opt]
            else:
                db_wids = [d.get("webid") for d in self._read_assetdatabases_table() if d.get("webid")]

            for db_wid in db_wids:
                try:
                    data = self._get_json(f"/piwebapi/assetdatabases/{db_wid}/analysistemplates")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append(
                        {
                            "webid": wid,
                            "name": it.get("Name", ""),
                            "description": it.get("Description", ""),
                            "path": it.get("Path", ""),
                            "assetdatabase_webid": db_wid,
                        }
                    )
            return out


        def _read_eventframe_template_attributes_table(self, table_options: Dict[str, str]) -> List[dict]:
            """List event frame template attribute templates (best-effort)."""
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()
            tpl_wid_opt = (table_options.get("eventframe_template_webid") or "").strip()

            template_pairs: List[Tuple[str, str]] = []
            if tpl_wid_opt:
                template_pairs = [(tpl_wid_opt, db_wid_opt)]
            else:
                # enumerate templates across DBs
                templates = self._read_eventframe_templates_table({"assetdatabase_webid": db_wid_opt} if db_wid_opt else {})
                for t in templates:
                    tw = t.get("webid")
                    if not tw:
                        continue
                    template_pairs.append((tw, t.get("assetdatabase_webid") or db_wid_opt))

            for tpl_wid, db_wid in template_pairs:
                try:
                    data = self._get_json(f"/piwebapi/eventframetemplates/{tpl_wid}/attributetemplates")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append(
                        {
                            "webid": wid,
                            "name": it.get("Name", ""),
                            "description": it.get("Description", ""),
                            "path": it.get("Path", ""),
                            "type": it.get("Type", "") or it.get("AttributeType", ""),
                            "eventframe_template_webid": tpl_wid,
                            "assetdatabase_webid": db_wid or "",
                        }
                    )
            return out


        def _read_streamset_end(self, table_options: Dict[str, str]) -> List[dict]:
            """Multi-tag end values (best-effort)."""
            tag_webids = self._resolve_tag_webids(table_options)
            ingest_ts = _utcnow()
            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]

            out: List[dict] = []
            # Prefer StreamSet endpoint if available
            for group in groups:
                if not group:
                    continue
                params: List[Tuple[str, str]] = [("webId", w) for w in group]
                try:
                    data = self._get_json("/piwebapi/streamsets/end", params=params)
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        data = None
                    else:
                        raise
                if data:
                    for stream in data.get("Items", []) or []:
                        wid = stream.get("WebId")
                        if not wid:
                            continue
                        v = stream.get("Value", {}) or {}
                        ts = v.get("Timestamp")
                        out.append(
                            {
                                "tag_webid": wid,
                                "timestamp": _parse_ts(ts) if ts else None,
                                "value": _try_float(v.get("Value")),
                                "good": _as_bool(v.get("Good"), default=True),
                                "questionable": _as_bool(v.get("Questionable"), default=False),
                                "substituted": _as_bool(v.get("Substituted"), default=False),
                                "annotated": _as_bool(v.get("Annotated"), default=False),
                                "units": v.get("UnitsAbbreviation", ""),
                                "ingestion_timestamp": ingest_ts,
                            }
                        )
                    continue

                # Fallback: per-tag /end
                for wid in group:
                    try:
                        v = self._get_json(f"/piwebapi/streams/{wid}/end")
                    except requests.exceptions.HTTPError as e:
                        if getattr(e.response, "status_code", None) == 404:
                            continue
                        raise
                    ts = v.get("Timestamp")
                    out.append(
                        {
                            "tag_webid": wid,
                            "timestamp": _parse_ts(ts) if ts else None,
                            "value": _try_float(v.get("Value")),
                            "good": _as_bool(v.get("Good"), default=True),
                            "questionable": _as_bool(v.get("Questionable"), default=False),
                            "substituted": _as_bool(v.get("Substituted"), default=False),
                            "annotated": _as_bool(v.get("Annotated"), default=False),
                            "units": v.get("UnitsAbbreviation", ""),
                            "ingestion_timestamp": ingest_ts,
                        }
                    )
            return out


        def _read_element_template_attributes_table(self, table_options: Dict[str, str]) -> List[dict]:
            """Richer attribute template inventory (best-effort)."""
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()
            tpl_wid_opt = (table_options.get("elementtemplate_webid") or "").strip()

            template_pairs: List[Tuple[str, str]] = []
            if tpl_wid_opt:
                template_pairs = [(tpl_wid_opt, db_wid_opt)]
            else:
                templates = self._read_element_templates_table({"assetdatabase_webid": db_wid_opt} if db_wid_opt else {})
                for t in templates:
                    tw = t.get("webid")
                    if not tw:
                        continue
                    template_pairs.append((tw, t.get("assetdatabase_webid") or db_wid_opt))

            for tpl_wid, db_wid in template_pairs:
                try:
                    data = self._get_json(f"/piwebapi/elementtemplates/{tpl_wid}/attributetemplates")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append(
                        {
                            "webid": wid,
                            "name": it.get("Name", ""),
                            "description": it.get("Description", ""),
                            "path": it.get("Path", ""),
                            "type": it.get("Type", "") or it.get("AttributeType", ""),
                            "default_units_name": it.get("DefaultUnitsName", "") or it.get("DefaultUnits", ""),
                            "data_reference_plugin": it.get("DataReferencePlugin", "") or it.get("DataReference", ""),
                            "is_configuration_item": _as_bool(it.get("IsConfigurationItem"), default=False),
                            "elementtemplate_webid": tpl_wid,
                            "assetdatabase_webid": db_wid or "",
                        }
                    )
            return out


        def _read_eventframe_referenced_elements(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            """EventFrame  referenced elements (best-effort)."""
            # Reuse the event frame windowing logic, and emit element relationships.
            it, next_offset = self._read_event_frames(start_offset, table_options)
            ingest_ts = _utcnow()

            def iterator() -> Iterator[dict]:
                for ef in it:
                    ef_wid = ef.get("event_frame_webid")
                    if not ef_wid:
                        continue
                    st = ef.get("start_time")
                    et = ef.get("end_time")
                    primary = ef.get("primary_referenced_element_webid")
                    if primary:
                        yield {
                            "event_frame_webid": ef_wid,
                            "element_webid": primary,
                            "relationship_type": "primary",
                            "start_time": st,
                            "end_time": et,
                            "ingestion_timestamp": ingest_ts,
                        }

                    # Optional: try API to enumerate more referenced elements (if supported)
                    try:
                        data = self._get_json(f"/piwebapi/eventframes/{ef_wid}/referencedelements")
                    except requests.exceptions.HTTPError as e:
                        if getattr(e.response, "status_code", None) == 404:
                            continue
                        raise
                    for el in (data.get("Items") or []):
                        ew = el.get("WebId")
                        if not ew or ew == primary:
                            continue
                        yield {
                            "event_frame_webid": ef_wid,
                            "element_webid": ew,
                            "relationship_type": "referenced",
                            "start_time": st,
                            "end_time": et,
                            "ingestion_timestamp": ingest_ts,
                        }

            return iterator(), next_offset


        def _read_af_tables_table(self, table_options: Dict[str, str]) -> List[dict]:
            """List AF Tables (best-effort)."""
            out: List[dict] = []
            db_wid_opt = (table_options.get("assetdatabase_webid") or "").strip()
            if db_wid_opt:
                db_wids = [db_wid_opt]
            else:
                db_wids = [d.get("webid") for d in self._read_assetdatabases_table() if d.get("webid")]

            for db_wid in db_wids:
                try:
                    data = self._get_json(f"/piwebapi/assetdatabases/{db_wid}/tables")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    if not wid:
                        continue
                    out.append(
                        {
                            "webid": wid,
                            "name": it.get("Name", ""),
                            "description": it.get("Description", ""),
                            "path": it.get("Path", ""),
                            "assetdatabase_webid": db_wid,
                        }
                    )
            return out


        def _read_af_table_rows_table(self, table_options: Dict[str, str]) -> List[dict]:
            """Read rows from AF Tables (best-effort).

            Options:
            - assetdatabase_webid (optional) to scope table discovery
            - table_webid (single) or table_webids (csv) to choose specific tables
            - default_tables: number of tables to sample if none specified (default 1)
            - startIndex/maxCount for row pagination (defaults 0/50)
            """
            start_index = int(table_options.get("startIndex", 0) or 0)
            max_count = int(table_options.get("maxCount", 50) or 50)
            default_tables = int(table_options.get("default_tables", 1) or 1)
            ingest_ts = _utcnow()

            table_webid = (table_options.get("table_webid") or "").strip()
            table_webids_csv = (table_options.get("table_webids") or "").strip()
            table_webids: List[str] = []
            if table_webid:
                table_webids = [table_webid]
            elif table_webids_csv:
                table_webids = [s.strip() for s in table_webids_csv.split(",") if s.strip()]
            else:
                tables = self._read_af_tables_table(table_options)
                table_webids = [t.get("webid") for t in tables if t.get("webid")][: max(0, default_tables)]

            out: List[dict] = []
            for tw in table_webids:
                try:
                    data = self._get_json(
                        f"/piwebapi/tables/{tw}/rows",
                        params={"startIndex": str(start_index), "maxCount": str(max_count)},
                    )
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    raise
                items = data.get("Items", []) or []
                for i, row in enumerate(items):
                    cols = row.get("Columns") or row.get("columns") or {}
                    # Normalize to string->string map for schema stability.
                    cols_norm = {str(k): ("" if v is None else str(v)) for k, v in (cols or {}).items()}
                    ridx = row.get("Index")
                    if ridx is None:
                        ridx = start_index + i
                    out.append(
                        {
                            "table_webid": tw,
                            "row_index": int(ridx) if str(ridx).isdigit() else None,
                            "columns": cols_norm,
                            "ingestion_timestamp": ingest_ts,
                        }
                    )
            return out


        def _read_eventframe_acknowledgements_table(self, table_options: Dict[str, str]) -> List[dict]:
            """Event frame acknowledgements (best-effort)."""
            ingest_ts = _utcnow()
            out: List[dict] = []

            # Use recent event frames as the driving set.
            try:
                it, _ = self._read_event_frames({}, table_options)
            except Exception as e:
                self._record_error(self.TABLE_EVENTFRAME_ACKS, "/piwebapi/assetdatabases/{db}/eventframes", e)
                return []

            max_elems = int(table_options.get("default_event_frames", 25) or 25)
            for i, ef in enumerate(it):
                if i >= max_elems:
                    break
                ef_wid = ef.get("event_frame_webid")
                if not ef_wid:
                    continue
                try:
                    data = self._get_json(f"/piwebapi/eventframes/{ef_wid}/acknowledgements")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    self._record_error(self.TABLE_EVENTFRAME_ACKS, f"/piwebapi/eventframes/{ef_wid}/acknowledgements", e)
                    continue
                for item in (data.get("Items") or []):
                    ack_id = item.get("Id") or item.get("WebId") or item.get("AckId")
                    if not ack_id:
                        continue
                    ts = item.get("Timestamp")
                    out.append(
                        {
                            "event_frame_webid": ef_wid,
                            "ack_id": str(ack_id),
                            "ack_timestamp": _parse_ts(ts) if ts else None,
                            "ack_user": item.get("User") or item.get("AcknowledgedBy") or "",
                            "comment": item.get("Comment") or "",
                            "ingestion_timestamp": ingest_ts,
                        }
                    )
            return out


        def _read_eventframe_annotations_table(self, table_options: Dict[str, str]) -> List[dict]:
            """Event frame annotations (best-effort)."""
            ingest_ts = _utcnow()
            out: List[dict] = []

            try:
                it, _ = self._read_event_frames({}, table_options)
            except Exception as e:
                self._record_error(self.TABLE_EVENTFRAME_ANNOTATIONS, "/piwebapi/assetdatabases/{db}/eventframes", e)
                return []

            max_elems = int(table_options.get("default_event_frames", 25) or 25)
            for i, ef in enumerate(it):
                if i >= max_elems:
                    break
                ef_wid = ef.get("event_frame_webid")
                if not ef_wid:
                    continue
                try:
                    data = self._get_json(f"/piwebapi/eventframes/{ef_wid}/annotations")
                except requests.exceptions.HTTPError as e:
                    if getattr(e.response, "status_code", None) == 404:
                        continue
                    self._record_error(self.TABLE_EVENTFRAME_ANNOTATIONS, f"/piwebapi/eventframes/{ef_wid}/annotations", e)
                    continue
                for item in (data.get("Items") or []):
                    ann_id = item.get("Id") or item.get("WebId") or item.get("AnnotationId")
                    if not ann_id:
                        continue
                    ts = item.get("Timestamp")
                    out.append(
                        {
                            "event_frame_webid": ef_wid,
                            "annotation_id": str(ann_id),
                            "annotation_timestamp": _parse_ts(ts) if ts else None,
                            "annotation_user": item.get("User") or item.get("CreatedBy") or "",
                            "text": item.get("Text") or item.get("Message") or "",
                            "ingestion_timestamp": ingest_ts,
                        }
                    )
            return out


        def _read_recorded_at_time(self, table_options: Dict[str, str]) -> List[dict]:
            """RecordedAtTime per tag (best-effort)."""
            tag_webids = self._resolve_tag_webids(table_options)
            time_param = table_options.get("time") or "*"
            ingest_ts = _utcnow()

            out: List[dict] = []
            for wid in tag_webids:
                try:
                    data = self._get_json(f"/piwebapi/streams/{wid}/recordedattime", params={"time": str(time_param)})
                except requests.exceptions.HTTPError as e:
                    # Fallback: use Stream GetValue if RecordedAtTime is unavailable on the source host.
                    if getattr(e.response, "status_code", None) == 404:
                        try:
                            data = self._get_json(f"/piwebapi/streams/{wid}/value", params={"time": str(time_param)})
                        except requests.exceptions.HTTPError as e2:
                            if getattr(e2.response, "status_code", None) == 404:
                                continue
                            self._record_error(self.TABLE_RECORDED_AT_TIME, f"/piwebapi/streams/{wid}/value", e2)
                            continue
                    else:
                        self._record_error(self.TABLE_RECORDED_AT_TIME, f"/piwebapi/streams/{wid}/recordedattime", e)
                        continue

                ts = data.get("Timestamp")
                # query_time is the requested time; parse best-effort
                try:
                    qt = _parse_pi_time(str(time_param), now=_utcnow())
                except Exception:
                    qt = None
                out.append(
                    {
                        "tag_webid": wid,
                        "query_time": qt,
                        "timestamp": _parse_ts(ts) if ts else None,
                        "value": _try_float(data.get("Value")),
                        "good": _as_bool(data.get("Good"), default=True),
                        "questionable": _as_bool(data.get("Questionable"), default=False),
                        "substituted": _as_bool(data.get("Substituted"), default=False),
                        "annotated": _as_bool(data.get("Annotated"), default=False),
                        "units": data.get("UnitsAbbreviation", ""),
                        "ingestion_timestamp": ingest_ts,
                    }
                )
            return out


        def _read_calculated(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            """Calculated values over time (best-effort)."""
            tag_webids = self._resolve_tag_webids(table_options)
            calc_type = (table_options.get("calculationType") or table_options.get("calculation_type") or "Average").strip()
            now = _utcnow()

            end_opt = table_options.get("endTime") or table_options.get("end_time") or "*"
            end_dt = _parse_pi_time(end_opt, now=now)

            start_dt: Optional[datetime] = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start_dt = _parse_ts(off)
                    except Exception:
                        start_dt = None

            if start_dt is None:
                start_opt = table_options.get("startTime") or table_options.get("start_time")
                if start_opt:
                    start_dt = _parse_pi_time(str(start_opt), now=end_dt)
                else:
                    lookback_minutes = int(table_options.get("lookback_minutes", 60))
                    start_dt = end_dt - timedelta(minutes=lookback_minutes)

            start_str = _isoformat_z(start_dt)
            end_str = _isoformat_z(end_dt)
            interval = (table_options.get("interval") or table_options.get("sampleInterval") or "1m").strip()
            ingest_ts = _utcnow()
            next_offset = {"offset": end_str}

            def iterator() -> Iterator[dict]:
                for wid in tag_webids:
                    try:
                        data = self._get_json(
                            f"/piwebapi/streams/{wid}/calculated",
                            params={"startTime": start_str, "endTime": end_str, "interval": interval, "calculationType": calc_type},
                        )
                    except requests.exceptions.HTTPError as e:
                        if getattr(e.response, "status_code", None) == 404:
                            # fallback: use plot points and label them as calculated
                            try:
                                data = self._get_json(
                                    f"/piwebapi/streams/{wid}/plot",
                                    params={"startTime": start_str, "endTime": end_str, "intervals": "120"},
                                )
                            except requests.exceptions.HTTPError as e2:
                                if getattr(e2.response, "status_code", None) == 404:
                                    continue
                                self._record_error(self.TABLE_CALCULATED, f"/piwebapi/streams/{wid}/plot", e2)
                                continue
                        else:
                            self._record_error(self.TABLE_CALCULATED, f"/piwebapi/streams/{wid}/calculated", e)
                            continue

                    for item in (data.get("Items") or []):
                        ts = item.get("Timestamp")
                        if not ts:
                            continue
                        yield {
                            "tag_webid": wid,
                            "timestamp": _parse_ts(ts),
                            "value": _try_float(item.get("Value")),
                            "units": item.get("UnitsAbbreviation", "") or item.get("Units", ""),
                            "calculation_type": calc_type,
                            "ingestion_timestamp": ingest_ts,
                        }

            return iterator(), next_offset


        def _read_point_type_catalog(self, table_options: Dict[str, str]) -> List[dict]:
            """Derive a point type catalog by scanning points and grouping by a coarse 'type'."""
            # We keep this lightweight: use pi_points results in-memory.
            points = self._read_points(table_options)
            counts: Dict[Tuple[str, str], int] = {}
            for p in points:
                desc = (p.get("descriptor") or "").strip()
                eu = (p.get("engineering_units") or "").strip()
                # heuristic: first word of descriptor (Temperature/Pressure/etc), else "Unknown"
                pt = (desc.split(" ", 1)[0] if desc else "Unknown") or "Unknown"
                key = (pt, eu)
                counts[key] = counts.get(key, 0) + 1
            out: List[dict] = []
            for (pt, eu), n in sorted(counts.items(), key=lambda x: (-x[1], x[0][0], x[0][1])):
                out.append({"point_type": pt, "engineering_units": eu, "count_points": n})
            return out


        def _read_links(self, table_options: Dict[str, str]) -> List[dict]:
            """Materialize Links fields (best-effort) by sampling key resources."""
            out: List[dict] = []

            def add(entity_type: str, webid: str, links: Any):
                if not webid or not isinstance(links, dict):
                    return
                for rel, href in links.items():
                    if href is None:
                        continue
                    out.append({"entity_type": entity_type, "webid": webid, "rel": str(rel), "href": str(href)})

            # Dataservers
            try:
                data = self._get_json("/piwebapi/dataservers")
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    add("dataserver", wid, it.get("Links"))
            except Exception:
                pass

            # AssetServers
            try:
                data = self._get_json("/piwebapi/assetservers")
                for it in (data.get("Items") or []):
                    wid = it.get("WebId")
                    add("assetserver", wid, it.get("Links"))
            except Exception:
                pass

            # AF Tables (inventory)
            try:
                for t in self._read_af_tables_table(table_options):
                    # we only have parsed fields; Links aren't preserved, so just emit a derived link.
                    wid = t.get("webid")
                    if wid:
                        out.append({"entity_type": "aftable", "webid": wid, "rel": "rows", "href": f"/piwebapi/tables/{wid}/rows"})
            except Exception:
                pass

            return out


        def _read_errors(self, table_options: Dict[str, str]) -> List[dict]:
            # Return the current buffered errors (best-effort).
            return list(self._errors)


        def _read_current_value(self, table_options: Dict[str, str]) -> List[dict]:
            tag_webids = self._resolve_tag_webids(table_options)
            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            tag_webid_groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]
            time_param = table_options.get("time")

            reqs: List[dict] = []
            for w in tag_webids:
                params: Dict[str, str] = {}
                if time_param:
                    params["time"] = str(time_param)
                reqs.append({"Method": "GET", "Resource": f"/piwebapi/streams/{w}/value", "Parameters": params})

            ingest_ts = _utcnow()
            out: List[dict] = []

            for group in tag_webid_groups:
                if not group:
                    continue
                group_reqs: List[dict] = []
                for w in group:
                    params: Dict[str, str] = {}
                    if time_param:
                        params["time"] = str(time_param)
                    group_reqs.append({"Method": "GET", "Resource": f"/piwebapi/streams/{w}/value", "Parameters": params})
                responses = self._batch_execute(group_reqs)
                for idx, (_rid, resp) in enumerate(responses):
                    if resp.get("Status") != 200:
                        continue
                    webid = group[idx] if idx < len(group) else None
                    if not webid:
                        continue
                    v = resp.get("Content", {}) or {}
                    ts = v.get("Timestamp")
                    out.append({
                        "tag_webid": webid,
                        "timestamp": _parse_ts(ts) if ts else None,
                        "value": _try_float(v.get("Value")),
                        "good": _as_bool(v.get("Good"), default=True),
                        "questionable": _as_bool(v.get("Questionable"), default=False),
                        "substituted": _as_bool(v.get("Substituted"), default=False),
                        "annotated": _as_bool(v.get("Annotated"), default=False),
                        "units": v.get("UnitsAbbreviation", ""),
                        "ingestion_timestamp": ingest_ts,
                    })

            return out

        def _read_summary(self, table_options: Dict[str, str]) -> List[dict]:
            # NOTE: The PI Web API supports multiple instances of summaryType.
            # We implement the API-correct approach (repeat summaryType in query params) and keep this table snapshot-like.
            tag_webids = self._resolve_tag_webids(table_options)
            tags_per_request = int(table_options.get("tags_per_request", 0) or 0)
            tag_webid_groups = _chunks(tag_webids, tags_per_request) if tags_per_request else [tag_webids]
            start_time = table_options.get("startTime")
            end_time = table_options.get("endTime")
            summary_types_csv = table_options.get("summaryType", "Total")
            summary_types = [s.strip() for s in str(summary_types_csv).split(",") if s.strip()]
            if not summary_types:
                summary_types = ["Total"]

            passthrough_keys = ("calculationBasis", "timeType", "summaryDuration", "sampleType", "sampleInterval", "timeZone", "filterExpression")
            passthrough = {k: str(table_options.get(k)) for k in passthrough_keys if table_options.get(k) is not None}

            ingest_ts = _utcnow()
            out: List[dict] = []

            for w in tag_webids:
                params: List[Tuple[str, str]] = []
                if start_time:
                    params.append(("startTime", str(start_time)))
                if end_time:
                    params.append(("endTime", str(end_time)))
                for st in summary_types:
                    params.append(("summaryType", st))
                for k, v in passthrough.items():
                    params.append((k, v))

                data = self._get_json(f"/piwebapi/streams/{w}/summary", params=params)
                for item in data.get("Items", []) or []:
                    stype = item.get("Type") or ""
                    v = item.get("Value", {}) or {}
                    ts = v.get("Timestamp")
                    out.append({
                        "tag_webid": w,
                        "summary_type": str(stype),
                        "timestamp": _parse_ts(ts) if ts else None,
                        "value": _try_float(v.get("Value")),
                        "good": _as_bool(v.get("Good"), default=True),
                        "questionable": _as_bool(v.get("Questionable"), default=False),
                        "substituted": _as_bool(v.get("Substituted"), default=False),
                        "annotated": _as_bool(v.get("Annotated"), default=False),
                        "units": v.get("UnitsAbbreviation", ""),
                        "ingestion_timestamp": ingest_ts,
                    })

            return out

        def _read_assetservers(self) -> List[dict]:
            data = self._get_json("/piwebapi/assetservers")
            return data.get("Items", []) or []

        def _read_assetdatabases(self, assetserver_webid: str) -> List[dict]:
            data = self._get_json(f"/piwebapi/assetservers/{assetserver_webid}/assetdatabases")
            return data.get("Items", []) or []

        def _read_af_hierarchy(self) -> List[dict]:
            assetservers = self._read_assetservers()
            if not assetservers:
                return []

            out: List[dict] = []
            ingest_ts = _utcnow()

            def walk(elements: List[dict], parent_webid: str, depth: int):
                for e in elements:
                    webid = e.get("WebId")
                    if not webid:
                        continue
                    out.append({
                        "element_webid": webid,
                        "name": e.get("Name", ""),
                        "template_name": e.get("TemplateName", ""),
                        "description": e.get("Description", ""),
                        "path": e.get("Path", ""),
                        "parent_webid": parent_webid or "",
                        "depth": depth,
                        "category_names": e.get("CategoryNames") or [],
                        "ingestion_timestamp": ingest_ts,
                    })
                    children = e.get("Elements") or []
                    if children:
                        walk(children, webid, depth + 1)

            for srv in assetservers:
                srv_webid = srv.get("WebId")
                if not srv_webid:
                    continue
                for db in self._read_assetdatabases(srv_webid):
                    db_webid = db.get("WebId")
                    if not db_webid:
                        continue
                    roots = self._get_json(
                        f"/piwebapi/assetdatabases/{db_webid}/elements",
                        params={"searchFullHierarchy": "true"},
                    ).get("Items", []) or []
                    walk(roots, parent_webid="", depth=0)

            return out

        def _read_event_frames(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            now = _utcnow()

            start = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start = _parse_ts(off)
                    except Exception:
                        start = None
            if start is None:
                lookback_days = int(table_options.get("lookback_days", 30))
                start = now - timedelta(days=lookback_days)

            start_str = _isoformat_z(start)
            end_str = _isoformat_z(now)

            page_size = int(table_options.get("maxCount", 1000))
            base_start_index = int(table_options.get("startIndex", 0))
            search_mode = table_options.get("searchMode", "Overlapped")

            assetservers = self._read_assetservers()
            if not assetservers:
                return iter(()), {"offset": end_str}

            all_events: List[dict] = []

            for srv in assetservers:
                srv_webid = srv.get("WebId")
                if not srv_webid:
                    continue
                for db in self._read_assetdatabases(srv_webid):
                    db_webid = db.get("WebId")
                    if not db_webid:
                        continue

                    start_index = base_start_index
                    while True:
                        params = {
                            "startTime": start_str,
                            "endTime": end_str,
                            "searchMode": str(search_mode),
                            "startIndex": str(start_index),
                            "maxCount": str(page_size),
                        }
                        resp = self._get_json(f"/piwebapi/assetdatabases/{db_webid}/eventframes", params=params)
                        items = resp.get("Items", []) or []
                        all_events.extend(items)
                        if len(items) < page_size:
                            break
                        start_index += page_size

            ingest_ts = _utcnow()

            def iterator() -> Iterator[dict]:
                for ef in all_events:
                    webid = ef.get("WebId")
                    if not webid:
                        continue
                    raw_attrs = ef.get("Attributes") or {}
                    attrs = {str(k): ("" if v is None else str(v)) for k, v in raw_attrs.items()}
                    yield {
                        "event_frame_webid": webid,
                        "name": ef.get("Name", ""),
                        "template_name": ef.get("TemplateName", ""),
                        "start_time": _parse_ts(ef.get("StartTime")) if ef.get("StartTime") else None,
                        "end_time": _parse_ts(ef.get("EndTime")) if ef.get("EndTime") else None,
                        "primary_referenced_element_webid": ef.get("PrimaryReferencedElementWebId"),
                        "description": ef.get("Description", ""),
                        "category_names": ef.get("CategoryNames") or [],
                        "attributes": attrs,
                        "ingestion_timestamp": ingest_ts,
                    }

            return iterator(), {"offset": end_str}

        def _read_element_attributes(self, table_options: Dict[str, str]) -> List[dict]:
            element_csv = table_options.get("element_webids", "")
            element_webids = [e.strip() for e in str(element_csv).split(",") if e.strip()]
            if not element_webids:
                af = self._read_af_hierarchy()
                element_webids = [r.get("element_webid") for r in af[: int(table_options.get("default_elements", 10))] if r.get("element_webid")]

            name_filter = table_options.get("nameFilter")
            page_size = int(table_options.get("maxCount", 1000))
            start_index = int(table_options.get("startIndex", 0))

            ingest_ts = _utcnow()
            out: List[dict] = []
            for ew in element_webids:
                params: Dict[str, str] = {"maxCount": str(page_size), "startIndex": str(start_index)}
                if name_filter:
                    params["nameFilter"] = str(name_filter)
                data = self._get_json(f"/piwebapi/elements/{ew}/attributes", params=params)
                for a in data.get("Items", []) or []:
                    aw = a.get("WebId")
                    if not aw:
                        continue
                    out.append({
                        "element_webid": ew,
                        "attribute_webid": aw,
                        "name": a.get("Name", ""),
                        "description": a.get("Description", ""),
                        "path": a.get("Path", ""),
                        "type": a.get("Type", ""),
                        "default_units_name": a.get("DefaultUnitsName", ""),
                        "data_reference_plugin": a.get("DataReferencePlugIn", ""),
                        "is_configuration_item": _as_bool(a.get("IsConfigurationItem"), default=False),
                        "ingestion_timestamp": ingest_ts,
                    })
            return out

        def _read_eventframe_attributes(self, table_options: Dict[str, str]) -> List[dict]:
            ef_csv = table_options.get("event_frame_webids", "")
            ef_webids = [e.strip() for e in str(ef_csv).split(",") if e.strip()]
            if not ef_webids:
                # sample event frames
                records, _ = self._read_event_frames({}, {"lookback_days": table_options.get("lookback_days", 30)})
                tmp = []
                for i, r in enumerate(records):
                    tmp.append(r)
                    if i >= int(table_options.get("default_event_frames", 10)) - 1:
                        break
                ef_webids = [r.get("event_frame_webid") for r in tmp if r.get("event_frame_webid")]

            name_filter = table_options.get("nameFilter")
            page_size = int(table_options.get("maxCount", 1000))
            start_index = int(table_options.get("startIndex", 0))

            ingest_ts = _utcnow()
            out: List[dict] = []
            for efw in ef_webids:
                params: Dict[str, str] = {"maxCount": str(page_size), "startIndex": str(start_index)}
                if name_filter:
                    params["nameFilter"] = str(name_filter)
                data = self._get_json(f"/piwebapi/eventframes/{efw}/attributes", params=params)
                for a in data.get("Items", []) or []:
                    aw = a.get("WebId")
                    if not aw:
                        continue
                    out.append({
                        "event_frame_webid": efw,
                        "attribute_webid": aw,
                        "name": a.get("Name", ""),
                        "description": a.get("Description", ""),
                        "path": a.get("Path", ""),
                        "type": a.get("Type", ""),
                        "default_units_name": a.get("DefaultUnitsName", ""),
                        "data_reference_plugin": a.get("DataReferencePlugIn", ""),
                        "is_configuration_item": _as_bool(a.get("IsConfigurationItem"), default=False),
                        "ingestion_timestamp": ingest_ts,
                    })
            return out


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
