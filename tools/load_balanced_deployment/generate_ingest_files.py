"""Generate multiple pipeline `ingest_*.py` entrypoints from a connector table CSV.

This is meant for load balancing across multiple Lakeflow pipelines by splitting
connector tables into categories (or any grouping key).

CSV format (header required):
  source_table,category,table_configuration_json

Example (OSIPI):
  tools/notebook_based_deployment/examples/osipi/osipi_tables_load_balancing.csv
"""

from __future__ import annotations

import argparse
import csv
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple


def _slug(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r"[^a-z0-9]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "group"


def _read_csv(path: Path) -> Dict[str, List[Tuple[str, Dict[str, str]]]]:
    if not path.exists():
        raise FileNotFoundError(str(path))

    out: Dict[str, List[Tuple[str, Dict[str, str]]]] = {}
    with path.open(newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        required = {"source_table", "category", "table_configuration_json"}
        missing = required - set(reader.fieldnames or [])
        if missing:
            raise ValueError(f"CSV missing required columns: {sorted(missing)}")

        for row in reader:
            source_table = (row.get("source_table") or "").strip()
            category = (row.get("category") or "").strip() or "default"
            cfg_raw = (row.get("table_configuration_json") or "").strip() or "{}"

            if not source_table:
                continue

            try:
                cfg_obj = json.loads(cfg_raw)
            except Exception as e:
                raise ValueError(
                    f"Invalid JSON in table_configuration_json for {source_table}: {cfg_raw}"
                ) from e

            if not isinstance(cfg_obj, dict):
                raise ValueError(
                    f"table_configuration_json must decode to an object/dict for {source_table}"
                )

            # Normalize to string->string for pipeline spec
            cfg: Dict[str, str] = {str(k): str(v) for k, v in cfg_obj.items()}
            out.setdefault(category, []).append((source_table, cfg))

    # Stable output order
    for k in list(out.keys()):
        out[k] = sorted(out[k], key=lambda x: x[0])
    return dict(sorted(out.items(), key=lambda kv: kv[0]))


def _emit_ingest_py(
    *,
    source_name: str,
    connection_name: str,
    catalog: str,
    schema: str,
    common_table_config: Dict[str, str],
    tables: List[Tuple[str, Dict[str, str]]],
) -> str:
    return f'''# Databricks notebook source
from pipeline.ingestion_pipeline import ingest
from libs.source_loader import get_register_function

get_register_function({source_name!r})(spark)

COMMON_TABLE_CONFIG = {common_table_config!r}

TABLES = {tables!r}

pipeline_spec = {{
    "connection_name": {connection_name!r},
    "objects": [
        {{
            "table": {{
                "source_table": source_table,
                "destination_catalog": {catalog!r},
                "destination_schema": {schema!r},
                "table_configuration": {{**COMMON_TABLE_CONFIG, **table_cfg}},
            }}
        }}
        for (source_table, table_cfg) in TABLES
    ],
}}

ingest(spark, pipeline_spec)
'''


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="Path to tables CSV")
    ap.add_argument("--output-dir", required=True, help="Directory to write ingest_*.py files")
    ap.add_argument("--source-name", required=True, help="Connector source name (e.g. osipi, hubspot, zendesk)")
    ap.add_argument("--connection-name", required=True, help="UC connection name used by the pipeline")
    ap.add_argument("--catalog", required=True, help="Destination catalog")
    ap.add_argument("--schema", required=True, help="Destination schema")
    ap.add_argument(
        "--common-table-config-json",
        required=True,
        help="JSON object (string->string) merged into each table_configuration",
    )
    args = ap.parse_args()

    csv_path = Path(args.csv)
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    grouped = _read_csv(csv_path)
    if not grouped:
        raise SystemExit("No tables found in CSV")

    common_cfg = json.loads(args.common_table_config_json)
    if not isinstance(common_cfg, dict):
        raise SystemExit("--common-table-config-json must be a JSON object")
    common_cfg = {str(k): str(v) for k, v in common_cfg.items()}

    for category, tables in grouped.items():
        fname = out_dir / f"ingest_{_slug(category)}.py"
        fname.write_text(
            _emit_ingest_py(
                source_name=args.source_name,
                connection_name=args.connection_name,
                catalog=args.catalog,
                schema=args.schema,
                common_table_config=common_cfg,
                tables=tables,
            ),
            encoding="utf-8",
        )

    print(f"Wrote {len(grouped)} ingest files to {out_dir}")


if __name__ == "__main__":
    main()
